# Local *ChatGPT-like* server

Setup a local ChatGPT like server to learn AI without the costs of ChatGPT
subscription.

This is a personal option to learn AI concepts; it does not replace (by any
strech of the imagination) using ChatGPT, nor is the intention of this post to
suggest you can replace ChatGPT so easily.

## Available Hardware

| Device | &nbsp;&nbsp;&nbsp;&nbsp; | Info |
| ---: | :---: | :--- |
| Computer || Dell XPS-8950 |
| CPU || 12th Gen Intel(R) Core(TM) i5-12600K |
| Memory || 128 GiB |
| Disk (OS) || Micron 2450 NVMe (256GB) |
| Disk (data) || KINGSTON SNV3S2000G  (1863GiB ~2TB)|
| GPU || NVIDIA GeForce RTX 4060 Ti |
| || - VRAM: 16GiB |
| || - CUDA: 12.8 |
| || - Driver: 570.195.03 |

## Architecture

(Generated by Copilot)

- Model Backend (LLM runtime) options:
  - **Ollama** → Simplest way to run models locally with GPU acceleration.
  - **vLLM** → High‑performance inference engine optimized for serving LLMs.
  - **Text Generation WebUI** → Flexible, but more hobbyist‑oriented.

- Model Weights:
  - **LLaMA‑3 8B** (Meta)
  - **Mistral 7B**
  - **Gemma 7B** (Google)
  - **Falcon 7B** (TII)

- API Layer:
  - **FastAPI** or **Flask** → lightweight Python servers.
  - **Ollama API** → already provides endpoints.
  - **OpenAI Forwarder** → wrappers that mimic OpenAI’s API spec.

## Selections

| Architecture Item | Choice |
| :---: | :--- |
| Model Backend | **vLLM** |
| Model Weights | **Mistral 7B** |
| API Layer | **FastAPI** |

## Pre-requisites

- Conda-Forge installed for all users
- GPU environment established for all users and frozen (no user should be able
to accidentally disrupt the NVIDIA drivers and compiler (nvcc) version).

## Setup

### Install Nginx

- `sudo apt install nginx -y`
- `nginx -v` → to verify installation

#### Enable Nginx as service and start it

- `sudo systemctl start nginx`
- `sudo systemctl enable nginx`

### Install certbot

- `sudo apt -y install certbot python3-certbot-nginx`

### Create Conda environment and activate it

- `conda env create -f ./environment.yml`
- `conda activate chatgpt-like-local-pi312-cu128`

### Download models

- `./utils/download_models.sh`

#### Create soft links to the models

- `ln -s /models/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/<snapshot-id> /models/ph3-mini-12k-local`
- `ln -s /models/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/<snapshot-id> /models/ph3-mini-4k-local`
- `ln -s /models/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/<snapshot-id> /models/mistral-7b-local`

### Test Mistral model

#### Open shell

- > Open New Shell: `python -m vllm.entrypoints.openai.api_server --served-model-name mistral-7b-local --model "/models/mistral-7b-local" --port 8001 --max-model-len 2048 --gpu-memory-utilization 0.95`
- > Open New Shell: `curl http://localhost:8001/v1/completions   -H "Content-Type: application/json"   -d '{ "model": "mistral-7b-local", "prompt": "playing bridge, I have a hand with 16 HCP, 5 spades, 4 hearts, 3 diamonds, 1 club. How should I open the hand in first position?", "max_tokens": 128 }'`
  - Expected results:

```txt
        {
            "id": "cmpl-b15521448291b401",
            "object": "text_completion",
            "created": 1765253153,
            "model": "mistral-7b-local",
            "choices": [
                {
                    "index": 0,
                    "text": "\n\nWhen you have a balanced hand with 16 HCP, you typically open the bidding with 1 flat of the suit you have the longest and strongest suit. In this case, you have 5 spades and 4 hearts, so you would open with 1 spade.\n\nHowever, since you mention that you're playing bridge, it's also important to consider the partnership bidding system your group uses. For example, some players might open 1 of a major if they have 6 or more cards in that suit, while others might open with 1 of a major even with only",
                    "logprobs": null,
                    "finish_reason": "length",
                    "stop_reason": null,
                    "token_ids": null,
                    "prompt_logprobs": null,
                    "prompt_token_ids": null
                }
            ],
            "service_tier": null,
            "system_fingerprint": null,
            "usage": {
                "prompt_tokens": 43,
                "total_tokens": 171,
                "completion_tokens": 128,
                "prompt_tokens_details": null
            },
            "kv_transfer_params": null
        }
```

## Setup Service

### Create service user `llm`

- `sudo useradd -m -r -s /bin/bash llm`

### Enable Conda for user

- Login as user: `sudo -u llm bash`
- Change to user home directory: `cd ~`
- Make downloads folder: `mkdir downloads`
- Change to downloads folder: `cd downloads`
- Download Conda-Forge installer: `wget -O Miniforge3.sh "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"`
- Execute the installer: `bash Miniforge3.sh -b -p "${HOME}/miniforge3"`
- Initialize conda environment: `~/miniforge3/bin/conda init`
- Disable *Conda **base***: `conda config --set auto_activate_base false`
- Either logout and login as *llm* user, or source `.bashrc`: `. .bashrc`

### Create 'default' Conda environment for *llm* user

- From the project folder: `sudo cp {environment.yml,chatgpt-local.service,run_chatgpt_local_service.sh} /home/llm/`
- Make sure all files are available to *llm* user: `sudo chown -R llm:llm /home/llm`
- Login as user: `sudo -u llm bash`
- Change to user home directory: `cd ~`
- Create conda environment: `conda env create -f environment.yml`

### Create service, start it, and enable it

- Copy service file to services location: `sudo cp chatgpt-local.service /etc/systemd/system/`
- Reload services: `sudo systemctl daemon-reload`
- Enable service: `sudo systemctl enable chatgpt-local`
- Start service: `sudo systemctl start chatgpt-local`
- Check service status: `sudo systemctl status chatgpt-local`
- Check model is up and running: `curl http://localhost:12347/v1/models`

## Reboot and test

- `sudo reboot now`
- `sudo systemctl status chatgpt-local`
- If everything is OK, here are some prompts to check out the new service:
  - test prompts:
    - "Explain why the sum of the first n odd numbers equals n squared, step by step."
    - "Explain how a hash map works, with a simple code example."
    - "Write a haiku in English, then translate it into Spanish while keeping the poetic form."
    - "List pros and cons of listening to vynil records compared to listen to the same titles in CDs."
    - "Explain the differences between inorganic chemestry and organic chemestry, then summarize the explanation in Spanish."
  - Use the following command to test the service, replace *<prompt>* with the corresponding prompt:
    - `curl http://localhost:12347/v1/completions   -H "Content-Type: application/json"   -d '{ "model": "mistral-7b-local", "prompt": "<prompt>", "max_tokens": 512 }'`

## Local ChatGPT-like service GPU usage

### Model Weights: **Mistral-7B-Instruct-v0.3**

- VLLM Engine Core: **VRAM** = 15,884 GiB
- available: **VRAM** = 478 MiB
